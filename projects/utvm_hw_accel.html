<!DOCTYPE html>
<html class="projects_pg">

  <head>
    <link href="../css/all_pages.css" rel="stylesheet">
    <link rel="icon" href="imgs/girish_pics/favicon_img.png">
  </head>

  <body>
    <div class="content">

      <div class="profile_pics">
        <a href="../projects.html"><img src="../imgs/icon_pics/back_button.png"></a>
      </div>

      <h1></h1>

      <div class="writeup">

        <h3>Custom Hardware Accelerators for On-Embedded-Device Inference</h2>

        <p>
          Accelerators (GPU, TPU, etc.) have been shown to significantly improve training and inference speeds for conventional 
          deep learning. However, embedded platforms lack the necessary resources (power) to handle such advanced accelerators. 
          During the Summer and Fall of 2021 I interned at <a href="https://octoml.ai/" class="purple">OctoML</a> where I 
          helped develop a system to add custom hardware accelerators to an embedded device and run inference using OctoML's 
          uTVM compiler. This project is still a work in progress and has inspired my interest in efficient machine learning. 
          <br><br>
        </p>

        <h3>Publications</h2>
        <p>
          Work still in progress!
        </p>

        <h3>Additional Material</h2>
        <p>
          <a href="https://github.com/areusch/microtvm-blogpost-eval" class="purple">More about uTVM</a>
        </p>

        <h3>Collaborators</h2>
        <p>
          <a href="https://github.com/areusch" class="purple">Andrew Reusch</a>,
          <a href="https://www.jwfromm.com/" class="purple">Josh Fromm</a>,
          <a href="https://ubicomplab.cs.washington.edu/members/" class="purple">Shwetak Patel</a>,
        </p>

        <br><br>
        <br><br>
        <br><br>


      </div>

    </div>
  </body>
</html>
